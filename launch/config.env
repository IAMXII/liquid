# 通信参数
MASTER_ADDR=10.54.99.213
MASTER_PORT=29500

# 训练配置
NUM_NODES=2
GPUS_PER_NODE=8
WORLD_SIZE=$((NUM_NODES * GPUS_PER_NODE))

# 通用参数
TRAIN_FILE=liquid/train/train_mem.py
DEEPSPEED_CONFIG=scripts/zero3.json
RUN_ARGS="
  --deepspeed ${DEEPSPEED_CONFIG} \
  --model_name_or_path ./model/gemma-7b-addtoken \
  --version gemma \
  --data_path /hfdata \
  --shuffleseed 42 \
  --percentage 0.75 \
  --T2I_ratio 0.1 \
  --vq_resolution 256 \
  --image_folder ./data/ \
  --bf16 True \
  --lora_enable True \
  --output_dir ./debug_gemma2b_mixpretrain_stage1 \
  --run_name debug_gemma2b_mixpretrain_stage1 \
  --num_train_epochs 2 \
  --per_device_train_batch_size 2 \
  --per_device_eval_batch_size 2 \
  --gradient_accumulation_steps 4 \
  --save_strategy steps \
  --save_steps 2000 \
  --save_total_limit 1 \
  --learning_rate 2e-5 \
  --weight_decay 0.0 \
  --warmup_ratio 0.03 \
  --lr_scheduler_type cosine \
  --logging_steps 1 \
  --tf32 True \
  --model_max_length 16384 \
  --gradient_checkpointing True \
  --dataloader_num_workers 4 \
  --lazy_preprocess True \
  --report_to wandb
"
